{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A= 1/(1+np.exp(-Z))\n",
    "    cache= Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A= Z*(Z>0)\n",
    "    cache= Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(Z):\n",
    "    f, g= sigmoid(Z)\n",
    "    return f*(1-f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(Z):\n",
    "    Z[Z<=0] = 0\n",
    "    Z[Z>0] = 1\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, activation_cache):\n",
    "    Z= activation_cache\n",
    "    return dA*sigmoid_derivative(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, activation_cache):\n",
    "    Z= activation_cache\n",
    "    return dA*relu_derivative(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= 'titanic_train.csv'\n",
    "test_data=\"titanic_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Pclass                                               Name  \\\n",
      "PassengerId                                                              \n",
      "1                 3                            Braund, Mr. Owen Harris   \n",
      "2                 1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "3                 3                             Heikkinen, Miss. Laina   \n",
      "4                 1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
      "5                 3                           Allen, Mr. William Henry   \n",
      "\n",
      "                Sex   Age  SibSp  Parch            Ticket     Fare Cabin  \\\n",
      "PassengerId                                                                \n",
      "1              male  22.0      1      0         A/5 21171   7.2500   NaN   \n",
      "2            female  38.0      1      0          PC 17599  71.2833   C85   \n",
      "3            female  26.0      0      0  STON/O2. 3101282   7.9250   NaN   \n",
      "4            female  35.0      1      0            113803  53.1000  C123   \n",
      "5              male  35.0      0      0            373450   8.0500   NaN   \n",
      "\n",
      "            Embarked  \n",
      "PassengerId           \n",
      "1                  S  \n",
      "2                  C  \n",
      "3                  S  \n",
      "4                  S  \n",
      "5                  S  \n"
     ]
    }
   ],
   "source": [
    "X= pd.read_csv(data, index_col='PassengerId')\n",
    "X_test=pd.read_csv(test_data, index_col=\"PassengerId\")\n",
    "Y= X.Survived\n",
    "X=X.drop(['Survived'], axis=1)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols with missing:  ['Age', 'Cabin', 'Embarked']\n",
      "object cols:  ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n",
      "no. of unique entries:  {'Name': 891, 'Sex': 2, 'Ticket': 681, 'Cabin': 147, 'Embarked': 3}\n",
      "low_cardinality_cols:  ['Sex', 'Embarked']\n",
      "high_cardinality_cols:  ['Ticket', 'Cabin', 'Name']\n",
      "numerical_cols ['SibSp', 'Parch', 'Pclass', 'Age', 'Fare']\n"
     ]
    }
   ],
   "source": [
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X.columns if X[col].isnull().any()]\n",
    "print(\"cols with missing: \",cols_with_missing)\n",
    "object_cols = [col for col in X.columns if X[col].dtype == \"object\"]\n",
    "print(\"object cols: \",object_cols)\n",
    "object_nunique = list(map(lambda col: X[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "print(\"no. of unique entries: \", d)\n",
    "low_cardinality_cols = [col for col in object_cols if X[col].nunique() < 10]\n",
    "high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "print(\"low_cardinality_cols: \",low_cardinality_cols)\n",
    "print(\"high_cardinality_cols: \", high_cardinality_cols)\n",
    "numerical_cols= list(set(X.columns)-set(object_cols))\n",
    "print(\"numerical_cols\", numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= X.drop(high_cardinality_cols, axis=1)\n",
    "X_test= X_test.drop(high_cardinality_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = SimpleImputer(strategy='mean')\n",
    "categorical_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                                           ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols),\n",
    "                                                 ('cat', categorical_transformer, low_cardinality_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
      "PassengerId                                                      \n",
      "1                 3    male  22.0      1      0   7.2500        S\n",
      "2                 1  female  38.0      1      0  71.2833        C\n",
      "3                 3  female  26.0      0      0   7.9250        S\n",
      "4                 1  female  35.0      1      0  53.1000        S\n",
      "5                 3    male  35.0      0      0   8.0500        S\n",
      "...             ...     ...   ...    ...    ...      ...      ...\n",
      "887               2    male  27.0      0      0  13.0000        S\n",
      "888               1  female  19.0      0      0  30.0000        S\n",
      "889               3  female   NaN      1      2  23.4500        S\n",
      "890               1    male  26.0      0      0  30.0000        C\n",
      "891               3    male  32.0      0      0   7.7500        Q\n",
      "\n",
      "[891 rows x 7 columns]\n",
      "                    0         1         2             3         4         5  \\\n",
      "PassengerId                                                                   \n",
      "1            0.432793 -0.473674  0.827377 -5.924806e-01 -0.502445 -0.737695   \n",
      "2            0.432793 -0.473674 -1.566107  6.387890e-01  0.786845  1.355574   \n",
      "3           -0.474545 -0.473674  0.827377 -2.846632e-01 -0.488854  1.355574   \n",
      "4            0.432793 -0.473674 -1.566107  4.079260e-01  0.420730  1.355574   \n",
      "5           -0.474545 -0.473674  0.827377  4.079260e-01 -0.486337 -0.737695   \n",
      "...               ...       ...       ...           ...       ...       ...   \n",
      "887         -0.474545 -0.473674 -0.369365 -2.077088e-01 -0.386671 -0.737695   \n",
      "888         -0.474545 -0.473674 -1.566107 -8.233437e-01 -0.044381  1.355574   \n",
      "889          0.432793  2.008933  0.827377  4.374348e-15 -0.176263  1.355574   \n",
      "890         -0.474545 -0.473674 -1.566107 -2.846632e-01 -0.044381 -0.737695   \n",
      "891         -0.474545 -0.473674  0.827377  1.770629e-01 -0.492378 -0.737695   \n",
      "\n",
      "                    6         7         8         9  \n",
      "PassengerId                                          \n",
      "1            0.737695 -0.482043 -0.307562  0.615838  \n",
      "2           -1.355574  2.074505 -0.307562 -1.623803  \n",
      "3           -1.355574 -0.482043 -0.307562  0.615838  \n",
      "4           -1.355574 -0.482043 -0.307562  0.615838  \n",
      "5            0.737695 -0.482043 -0.307562  0.615838  \n",
      "...               ...       ...       ...       ...  \n",
      "887          0.737695 -0.482043 -0.307562  0.615838  \n",
      "888         -1.355574 -0.482043 -0.307562  0.615838  \n",
      "889         -1.355574 -0.482043 -0.307562  0.615838  \n",
      "890          0.737695  2.074505 -0.307562 -1.623803  \n",
      "891          0.737695 -0.482043  3.251373 -1.623803  \n",
      "\n",
      "[891 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "X_transformed= preprocessor.fit_transform(X)\n",
    "X_transformed= pd.DataFrame(StandardScaler().fit_transform(X_transformed))\n",
    "X_transformed.index= X.index\n",
    "print(X)\n",
    "print(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Pclass     Sex   Age  SibSp  Parch      Fare Embarked\n",
      "PassengerId                                                       \n",
      "892               3    male  34.5      0      0    7.8292        Q\n",
      "893               3  female  47.0      1      0    7.0000        S\n",
      "894               2    male  62.0      0      0    9.6875        Q\n",
      "895               3    male  27.0      0      0    8.6625        S\n",
      "896               3  female  22.0      1      1   12.2875        S\n",
      "...             ...     ...   ...    ...    ...       ...      ...\n",
      "1305              3    male   NaN      0      0    8.0500        S\n",
      "1306              1  female  39.0      0      0  108.9000        C\n",
      "1307              3    male  38.5      0      0    7.2500        S\n",
      "1308              3    male   NaN      0      0    8.0500        S\n",
      "1309              3    male   NaN      1      1   22.3583        C\n",
      "\n",
      "[418 rows x 7 columns]\n",
      "                    0         1         2             3         4         5  \\\n",
      "PassengerId                                                                   \n",
      "892         -0.499470 -0.400248  0.873482  3.349926e-01 -0.498407 -0.755929   \n",
      "893          0.616992 -0.400248  0.873482  1.325530e+00 -0.513274  1.322876   \n",
      "894         -0.499470 -0.400248 -0.315819  2.514175e+00 -0.465088 -0.755929   \n",
      "895         -0.499470 -0.400248  0.873482 -2.593299e-01 -0.483466 -0.755929   \n",
      "896          0.616992  0.619896  0.873482 -6.555448e-01 -0.418471  1.322876   \n",
      "...               ...       ...       ...           ...       ...       ...   \n",
      "1305        -0.499470 -0.400248  0.873482 -2.533749e-15 -0.494448 -0.755929   \n",
      "1306        -0.499470 -0.400248 -1.505120  6.915861e-01  1.313753  1.322876   \n",
      "1307        -0.499470 -0.400248  0.873482  6.519646e-01 -0.508792 -0.755929   \n",
      "1308        -0.499470 -0.400248  0.873482 -2.533749e-15 -0.494448 -0.755929   \n",
      "1309         0.616992  0.619896  0.873482 -2.533749e-15 -0.237906 -0.755929   \n",
      "\n",
      "                    6         7         8         9  \n",
      "PassengerId                                          \n",
      "892          0.755929 -0.568142  2.843757 -1.350676  \n",
      "893         -1.322876 -0.568142 -0.351647  0.740370  \n",
      "894          0.755929 -0.568142  2.843757 -1.350676  \n",
      "895          0.755929 -0.568142 -0.351647  0.740370  \n",
      "896         -1.322876 -0.568142 -0.351647  0.740370  \n",
      "...               ...       ...       ...       ...  \n",
      "1305         0.755929 -0.568142 -0.351647  0.740370  \n",
      "1306        -1.322876  1.760125 -0.351647 -1.350676  \n",
      "1307         0.755929 -0.568142 -0.351647  0.740370  \n",
      "1308         0.755929 -0.568142 -0.351647  0.740370  \n",
      "1309         0.755929  1.760125 -0.351647 -1.350676  \n",
      "\n",
      "[418 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "X_test_trans= preprocessor.fit_transform(X_test)\n",
    "X_test_trans= pd.DataFrame(StandardScaler().fit_transform(X_test_trans))\n",
    "X_test_trans.index= X_test.index\n",
    "print(X_test)\n",
    "print(X_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 418)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val= train_test_split(X_transformed, Y, test_size=0.3, random_state= 1)\n",
    "X_train=X_train.T\n",
    "X_val= X_val.T\n",
    "train_indices=Y_train.index\n",
    "val_indices=Y_val.index\n",
    "test_indices=X_test.index\n",
    "Y_train= np.array([Y_train])\n",
    "Y_val= np.array([Y_val])\n",
    "X_test= X_test_trans.T\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 891)\n",
      "(1, 891)\n"
     ]
    }
   ],
   "source": [
    "X= X_transformed.T\n",
    "Y=np.array([Y])\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/layers_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W,A)+b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters,keep_prob):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    DD=[]\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation=\"relu\")\n",
    "        D = np.random.rand(A.shape[0],A.shape[1])                   # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n",
    "        D = (D < keep_prob).astype(int)               # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n",
    "        DD.append(D)\n",
    "        A = np.multiply(A,D)                                         # Step 3: shut down some neurons of A1\n",
    "        A = A/keep_prob  \n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches, DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y, parameters):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    lambd=0.7\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = -sum(sum(np.multiply(Y,np.log(AL))+np.multiply(1-Y,np.log(1-AL))))/m \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost_with_regularization\n",
    "\n",
    "def compute_cost_with_regularization(AL, Y, parameters, cost, lambd):\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 1 line)\n",
    "    reg=0\n",
    "    for l in range(1,len(parameters)//2+1):\n",
    "        reg=reg+np.sum(np.square(parameters['W'+str(l)]))\n",
    "    L2_regularization_cost = lambd*reg/(2*m)\n",
    "    ### END CODER HERE ###\n",
    "    \n",
    "    cost = cost+L2_regularization_cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = np.dot(dZ,A_prev.T)/m  \n",
    "    db = np.sum(dZ, axis=1, keepdims=True )/m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches, DD, keep_prob):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL =  -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation= \"sigmoid\")\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines) \n",
    "        grads[\"dA\" + str(l+1)]= grads[\"dA\" + str(l+1)]*DD[l]/keep_prob\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation= \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backward_propagation_with_regularization\n",
    "\n",
    "def backward_propagation_with_regularization(X, grads, lambd):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    for l in range(1,len(grads)//3+1):\n",
    "        grads[\"dW\"+str(l)]=lambd*grads[\"dW\"+str(l)]/m\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] =  parameters[\"W\" + str(l+1)] -learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] -learning_rate*grads[\"db\" + str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate , num_iterations, keep_prob, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches, DD = L_model_forward(X, parameters, keep_prob)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y, parameters)\n",
    "        #cost = compute_cost_with_regularization(AL, Y, parameters, cost, lambd)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches, DD, keep_prob)\n",
    "        #grads = backward_propagation_with_regularization(X, grads, lambd)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (X,parameters):\n",
    "    L=len(parameters)//2\n",
    "    A=X\n",
    "    for l in  range (1,L):\n",
    "        Z= np.dot(parameters['W'+str(l)],A)+parameters['b'+str(l)]\n",
    "        A,temp= relu(Z)\n",
    "    Z= np.dot(parameters['W'+str(L)],A)+parameters['b'+str(L)]\n",
    "    A,temp= sigmoid(Z)\n",
    "    A=np.round(A)\n",
    "    return(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [10,6,6,6,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.727956\n",
      "Cost after iteration 100: 0.500910\n",
      "Cost after iteration 200: 0.487355\n",
      "Cost after iteration 300: 0.448952\n",
      "Cost after iteration 400: 0.463147\n",
      "Cost after iteration 500: 0.447075\n",
      "Cost after iteration 600: 0.439473\n",
      "Cost after iteration 700: 0.449445\n",
      "Cost after iteration 800: 0.440021\n",
      "Cost after iteration 900: 0.425448\n",
      "Cost after iteration 1000: 0.441463\n",
      "Cost after iteration 1100: 0.453338\n",
      "Cost after iteration 1200: 0.404848\n",
      "Cost after iteration 1300: 0.410778\n",
      "Cost after iteration 1400: 0.436536\n",
      "Cost after iteration 1500: 0.445800\n",
      "Cost after iteration 1600: 0.444133\n",
      "Cost after iteration 1700: 0.426333\n",
      "Cost after iteration 1800: 0.426857\n",
      "Cost after iteration 1900: 0.440438\n",
      "Cost after iteration 2000: 0.427758\n",
      "Cost after iteration 2100: 0.421690\n",
      "Cost after iteration 2200: 0.428134\n",
      "Cost after iteration 2300: 0.420440\n",
      "Cost after iteration 2400: 0.432067\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(X_train, Y_train,  layers_dims, learning_rate = 0.7, num_iterations = 2500, keep_prob=0.8 ,print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8603531300160514\n"
     ]
    }
   ],
   "source": [
    "#val accuracydata:image/png;base64,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\n",
    "pred= predict(X_train, parameters)\n",
    "accuracy= 1-mean_absolute_error(pred,Y_train)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7723880597014925\n"
     ]
    }
   ],
   "source": [
    "pred= predict(X_val,parameters)\n",
    "accuracy= 1-mean_absolute_error(pred,Y_val)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418,)\n"
     ]
    }
   ],
   "source": [
    "pred= predict(X_test, parameters)\n",
    "pred=pred.T\n",
    "pred=pred.reshape(-1)\n",
    "pred=pred.astype(int)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test predictions to file\n",
    "output = pd.DataFrame({'PassengerId': test_indices,\n",
    "                       'Survived': pred})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
